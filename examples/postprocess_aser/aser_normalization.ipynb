{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASER Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import argparse\n",
    "import normalization\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_personal_words(G: nx.DiGraph, show_possessive=True, n_info=10):\n",
    "    '''\n",
    "    TODO: \n",
    "    -how to utilize all the \"info\" of a given node? (currently only the first is used/sample \"n_info\" infos and use the one with largest freq)\n",
    "    -examine _add_edge, the coref-info!\n",
    "    '''\n",
    "    nmlzer = normalization.ParsingBasedNormalizer()\n",
    "    def _norm_node(node):\n",
    "        node_attr = G.nodes[node]\n",
    "\n",
    "        if isinstance(n_info, int) and n_info > 0:\n",
    "            sampled_infos = [eval(sampled_info) for sampled_info in sample(node_attr['info'], min(len(node_attr['info']), n_info))]\n",
    "            target_info = max(sampled_infos, key=lambda x: x['frequency'])\n",
    "        else:\n",
    "            # much faster\n",
    "            first_info = eval(node_attr['info'][0])\n",
    "            target_info = first_info\n",
    "\n",
    "        tmp = nmlzer.get_personal_words(target_info)\n",
    "        coref = nmlzer.node_person_coref(tmp, target_info)\n",
    "\n",
    "        res = nmlzer.get_norm_node(node, coref, show_possessive)\n",
    "        new_node, p2i = res['norm_node'], res['p2i']\n",
    "        return new_node, p2i, coref, target_info\n",
    "\n",
    "    def _add_node(G, G_norm, node, new_node, p2i, coref):\n",
    "        node_attr = G.nodes[node]\n",
    "\n",
    "        if G_norm.has_node(new_node): # update node freq & info\n",
    "            G_norm.nodes[new_node]['freq'] += node_attr['freq']\n",
    "            G_norm.nodes[new_node]['info'] = G_norm.nodes[new_node]['info'] | set(node_attr['info'])\n",
    "        else: # add new node to graph, add personal coref info\n",
    "            G_norm.add_node(new_node, freq=node_attr['freq'], info=set(node_attr['info']), \\\n",
    "                        people=coref, p2i=p2i)\n",
    "\n",
    "    def _merge_rel_dict(d1: dict, d2: dict):\n",
    "        d_merge = {}\n",
    "        for key in set(d1.keys()) | set(d2.keys()):\n",
    "            d_merge[key] = d1.get(key, 0) + d2.get(key, 0)\n",
    "        return d_merge\n",
    "\n",
    "    def _add_edge(G_norm, edge_attr, norm_head, norm_tail, pair_coref):\n",
    "        # TODO: coref update?\n",
    "        relations = edge_attr[\"relations\"]\n",
    "        if G_norm.has_edge(norm_head, norm_tail):\n",
    "            coreference = G_norm.edges[norm_head, norm_tail]['coreference']\n",
    "            coreference.add(str(pair_coref))    # update all pair coreference \n",
    "            G_norm.add_edge(norm_head, norm_tail,\n",
    "                            relations=_merge_rel_dict(G_norm[norm_head][norm_tail]['relations'], relations),\\\n",
    "                            coreference=coreference)\n",
    "        else:\n",
    "            G_norm.add_edge(norm_head, norm_tail, relations=relations, coreference={str(pair_coref)})\n",
    "\n",
    "    # process all nodes\n",
    "    node2new_info = {}\n",
    "    G_norm = nx.DiGraph()\n",
    "    print('Adding normalized nodes to new graph...')\n",
    "    for node in tqdm(G.nodes):\n",
    "        new_node, p2i, coref, info = _norm_node(node)\n",
    "        _add_node(G, G_norm, node, new_node, p2i, coref)\n",
    "        node2new_info[node] = (new_node, coref, info)\n",
    "\n",
    "    print('Adding edges to new graph...')\n",
    "    for head, tail, edge_attr in tqdm(G.edges.data()):\n",
    "        h_new_node, h_coref, h_info = node2new_info[head]\n",
    "        t_new_node, t_coref, t_info = node2new_info[tail]\n",
    "\n",
    "        # get pair coref\n",
    "        pair_coref = nmlzer.pair_person_coref(h_coref, t_coref, h_info, t_info)\n",
    "        _add_edge(G_norm, edge_attr, h_new_node, t_new_node, pair_coref)\n",
    "    \n",
    "    return G_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aser_path = '/home/data/jchengaj/aser_data/core_10.pickle'\n",
    "output_path = '/home/data/jchengaj/aser_data/core_10_normed_poss.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aser = nx.read_gpickle(aser_path)\n",
    "print('# node', len(aser.nodes))\n",
    "print('# edge', len(aser.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding normalized nodes to new graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6627568/47611301 [1:01:08<6:18:03, 1806.73it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40419/417348715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_personal_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_possessive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# node'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# edge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_gpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_40419/406090723.py\u001b[0m in \u001b[0;36mnormalize_personal_words\u001b[0;34m(G, show_possessive, n_info)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding normalized nodes to new graph...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnew_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_norm_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0m_add_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mnode2new_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_40419/406090723.py\u001b[0m in \u001b[0;36m_norm_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcoref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmlzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_person_coref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmlzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_norm_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_possessive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mnew_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm_node'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p2i'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hkust/ASER/examples/postprocess_aser/normalization.py\u001b[0m in \u001b[0;36mget_norm_node\u001b[0;34m(self, node, personal_coref, show_possessive)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mspans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# no overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mnode_wrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G_norm = normalize_personal_words(aser, show_possessive=True)\n",
    "print('# node', len(G_norm.nodes))\n",
    "print('# edge', len(G_norm.edges))\n",
    "nx.write_gpickle(G_norm, output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a6f00ce61866926d9fee9ef92c6f638e67bf6cadffeaa6df23d2879e3e484da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aser')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
