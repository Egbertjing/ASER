{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASER Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import argparse\n",
    "import normalization\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_personal_words(G: nx.DiGraph, show_possessive=True, n_info=10):\n",
    "    '''\n",
    "    TODO: \n",
    "    -how to utilize all the \"info\" of a given node? (currently only the first is used/sample \"n_info\" infos and use the one with largest freq)\n",
    "    -examine _add_edge, the coref-info!\n",
    "    '''\n",
    "    nmlzer = normalization.ParsingBasedNormalizer()\n",
    "    def _norm_node(node):\n",
    "        node_attr = G.nodes[node]\n",
    "\n",
    "        if isinstance(n_info, int) and n_info > 0:\n",
    "            sampled_infos = [eval(sampled_info) for sampled_info in sample(node_attr['info'], min(len(node_attr['info']), n_info))]\n",
    "            target_info = max(sampled_infos, key=lambda x: x['frequency'])\n",
    "        else:\n",
    "            # much faster\n",
    "            first_info = eval(node_attr['info'][0])\n",
    "            target_info = first_info\n",
    "\n",
    "        tmp = nmlzer.get_personal_words(target_info)\n",
    "        coref = nmlzer.node_person_coref(tmp, target_info)\n",
    "\n",
    "        res = nmlzer.get_norm_node(node, coref, show_possessive)\n",
    "        new_node, p2i = res['norm_node'], res['p2i']\n",
    "        return new_node, p2i, coref, target_info\n",
    "\n",
    "    def _add_node(G, G_norm, node, new_node, p2i, coref):\n",
    "        node_attr = G.nodes[node]\n",
    "\n",
    "        if G_norm.has_node(new_node): # update node freq & info\n",
    "            G_norm.nodes[new_node]['freq'] += node_attr['freq']\n",
    "            G_norm.nodes[new_node]['info'] = G_norm.nodes[new_node]['info'] | set(node_attr['info'])\n",
    "        else: # add new node to graph, add personal coref info\n",
    "            G_norm.add_node(new_node, freq=node_attr['freq'], info=set(node_attr['info']), \\\n",
    "                        people=coref, p2i=p2i)\n",
    "\n",
    "    def _merge_rel_dict(d1: dict, d2: dict):\n",
    "        d_merge = {}\n",
    "        for key in set(d1.keys()) | set(d2.keys()):\n",
    "            d_merge[key] = d1.get(key, 0) + d2.get(key, 0)\n",
    "        return d_merge\n",
    "\n",
    "    def _add_edge(G_norm, edge_attr, norm_head, norm_tail, pair_coref):\n",
    "        # TODO: coref update?\n",
    "        relations = edge_attr[\"relations\"]\n",
    "        if G_norm.has_edge(norm_head, norm_tail):\n",
    "            coreference = G_norm.edges[norm_head, norm_tail]['coreference']\n",
    "            coreference.add(str(pair_coref))    # update all pair coreference \n",
    "            G_norm.add_edge(norm_head, norm_tail,\n",
    "                            relations=_merge_rel_dict(G_norm[norm_head][norm_tail]['relations'], relations),\\\n",
    "                            coreference=coreference)\n",
    "        else:\n",
    "            G_norm.add_edge(norm_head, norm_tail, relations=relations, coreference={str(pair_coref)})\n",
    "\n",
    "    # process all nodes\n",
    "    node2new_info = {}\n",
    "    G_norm = nx.DiGraph()\n",
    "    print('Adding normalized nodes to new graph...')\n",
    "    for node in tqdm(G.nodes):\n",
    "        new_node, p2i, coref, info = _norm_node(node)\n",
    "        _add_node(G, G_norm, node, new_node, p2i, coref)\n",
    "        node2new_info[node] = (new_node, coref, info)\n",
    "\n",
    "    print('Adding edges to new graph...')\n",
    "    for head, tail, edge_attr in tqdm(G.edges.data()):\n",
    "        h_new_node, h_coref, h_info = node2new_info[head]\n",
    "        t_new_node, t_coref, t_info = node2new_info[tail]\n",
    "\n",
    "        # get pair coref\n",
    "        pair_coref = nmlzer.pair_person_coref(h_coref, t_coref, h_info, t_info)\n",
    "        _add_edge(G_norm, edge_attr, h_new_node, t_new_node, pair_coref)\n",
    "    \n",
    "    return G_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aser_path = '/home/data/jchengaj/aser_data/core_10.pickle'\n",
    "output_path = '/home/data/jchengaj/aser_data/core_10_normed_poss.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aser = nx.read_gpickle(aser_path)\n",
    "print('# node', len(aser.nodes))\n",
    "print('# edge', len(aser.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_norm = normalize_personal_words(aser, show_possessive=True)\n",
    "print('# node', len(G_norm.nodes))\n",
    "print('# edge', len(G_norm.edges))\n",
    "nx.write_gpickle(G_norm, output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a6f00ce61866926d9fee9ef92c6f638e67bf6cadffeaa6df23d2879e3e484da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aser')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
