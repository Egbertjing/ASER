from itertools import chain, permutations
from aser.eventuality import Eventuality
from aser.extract.rule import ALL_EVENTUALITY_RULES
# from aser.extract.rule import CONNECTIVE_LIST, CLAUSE_WORDS
from aser.extract.utils import parse_sentense_with_stanford, get_corenlp_client, get_clauses, powerset
from aser.extract.utils import ANNOTATORS
from aser.extract.discourse_parser import ConnectiveExtractor, ArgumentPositionClassifier, \
    SSArgumentExtractor, PSArgumentExtractor, ExplicitSenseClassifier
from aser.extract.discourse_parser import SyntaxTree

class BaseEventualityExtractor(object):
    def __init__(self, **kw):
        self.corenlp_path = kw.get("corenlp_path", "")
        self.corenlp_port = kw.get("corenlp_port", 0)
        self.annotators = kw.get("annotators", list(ANNOTATORS))

        _, self.is_externel_corenlp = get_corenlp_client(corenlp_path=self.corenlp_path, corenlp_port=self.corenlp_port)

    def close(self):
        if not self.is_externel_corenlp:
            corenlp_client, _ = get_corenlp_client(corenlp_path=self.corenlp_path, corenlp_port=self.corenlp_port)
            corenlp_client.stop()

    def __del__(self):
        self.close()

    def parse_text(self, text, annotators=None):
        if annotators is None:
            annotators = self.annotators

        corenlp_client, _ = get_corenlp_client(corenlp_path=self.corenlp_path, corenlp_port=self.corenlp_port, annotators=annotators)
        parsed_result = parse_sentense_with_stanford(text, corenlp_client, self.annotators)
        return parsed_result

    def extract_from_text(self, text, output_format="Eventuality", in_order=True, annotators=None, **kw):
        """ This method would firstly split text into sentences and extract
            all eventualities for each sentence.

            :type text: str
            :param text: input text
            :return: a list of lists of `Eventuality` object

            .. highlight:: python
            .. code-block:: python

                Input: 'The dog barks loudly. Because he is hungry.'

                Output:
                [
                    [Eventuality({'dependencies': [((1, 'dog', 'NN'), 'det', (0, 'the', 'DT')),
                                                 ((2, 'bark', 'VBZ'), 'nsubj', (1, 'dog', 'NN')),
                                                 ((2, 'bark', 'VBZ'), 'advmod', (3, 'loudly', 'RB'))],
                                 'eid': 'c605e3d855d7d27cf25ed7e4d4e33962d9b10713',
                                 'pattern': 's-v',
                                 'pos_tags': ['DT', 'NN', 'VBZ', 'RB'],
                                 'skeleton_dependencies': [((2, 'bark', 'VBZ'), 'nsubj', (1, 'dog', 'NN'))],
                                 'skeleton_words': ['dog', 'bark'],
                                 'verbs': ['bark'],
                                 'words': ['the', 'dog', 'bark', 'loudly']})],
                    [Eventuality({'dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'he', 'PRP')),
                                                  ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBZ'))],
                                  'eid': 'dae842ef792d3ae786db5f71e36b50305dda14a4',
                                  'pattern': 's-be-a',
                                  'pos_tags': ['PRP', 'VBZ', 'JJ'],
                                  'skeleton_dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'he', 'PRP')),
                                                            ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBZ'))],
                                  'skeleton_words': ['he', 'be', 'hungry'],
                                  'verbs': ['be'],
                                  'words': ['he', 'be', 'hungry']})]
                ]
        """
        if output_format not in ["Eventuality", "json"]:
            raise NotImplementedError("Error: extract_from_text only supports Eventuality or json.")
        parsed_result = self.parse_text(text, annotators)
        return self.extract_from_parsed_result(parsed_result, output_format, in_order, **kw)

    def extract_from_parsed_result(self, parsed_result, output_format="Eventuality", in_order=True, **kw):
        """ This method would extract eventualities from parsed_result of one sentence

        :type parsed_result: dict, or a list of dict
        :param parsed_result: a dict generated by `aser.extract.utils.parse_sentense_with_stanford` or a list of dict
        :return: a list of `Eventuality` objects

        .. highlight:: python
        .. code-block:: python

            Input:
                {'dependencies': [(1, 'nsubj', 0),
                                  (1, 'nmod:to', 3),
                                  (1, 'advcl:because', 7),
                                  (1, 'punct', 8),
                                  (3, 'case', 2),
                                  (7, 'mark', 4),
                                  (7, 'nsubj', 5),
                                  (7, 'cop', 6)],
                 'lemma': ['I', 'go', 'to', 'lunch', 'because', 'I', 'be', 'hungry', '.'],
                 'pos_tags': ['PRP', 'VBP', 'TO', 'NN', 'IN', 'PRP', 'VBP', 'JJ', '.'],
                 'tokens': ['I', 'go', 'to', 'lunch', 'because', 'I', 'am', 'hungry', '.']}

            Output:
                [Eventuality({'dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'I', 'PRP')),
                                    ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBP'))],
                                'eid': 'eae8741fad51a57e78092017def1b5cb4f620d7e',
                                'pattern': 's-be-a',
                                'pos_tags': ['PRP', 'VBP', 'JJ'],
                                'skeleton_dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'I', 'PRP')),
                                                        ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBP'))],
                                'skeleton_words': ['I', 'be', 'hungry'],
                                'verbs': ['be'],
                                'words': ['I', 'be', 'hungry']}),

                    Eventuality({'dependencies': [((1, 'go', 'VBP'), 'nsubj', (0, 'I', 'PRP')),
                                                ((1, 'go', 'VBP'), 'nmod:to', (3, 'lunch', 'NN')),
                                                ((3, 'lunch', 'NN'), 'case', (2, 'to', 'TO'))],
                                'eid': '12b4aa577e56f2f5d96f4716bc97c633d6272ec4',
                                'pattern': 's-v-X-o',
                                'pos_tags': ['PRP', 'VBP', 'TO', 'NN'],
                                'skeleton_dependencies': [((1, 'go', 'VBP'), 'nsubj', (0, 'I', 'PRP')),
                                                        ((1, 'go', 'VBP'), 'nmod:to', (3, 'lunch', 'NN')),
                                                        ((3, 'lunch', 'NN'), 'case', (2, 'to', 'TO'))],
                                'skeleton_words': ['I', 'go', 'to', 'lunch'],
                                'verbs': ['go'],
                                'words': ['I', 'go', 'to', 'lunch']})]
        """
        if output_format not in ["Eventuality", "json"]:
            raise NotImplementedError("Error: extract_from_parsed_result only supports Eventuality or json.")
        raise NotImplementedError


class SeedRuleEventualityExtractor(BaseEventualityExtractor):
    def __init__(self, **kw):
        super().__init__(**kw)

    def extract_from_parsed_result(self, parsed_result, output_format="Eventuality", in_order=True, **kw):
        if output_format not in ["Eventuality", "json"]:
            raise NotImplementedError("Error: extract_from_parsed_result only supports Eventuality or json.")

        if not isinstance(parsed_result, (list, tuple, dict)):
            raise NotImplementedError
        if isinstance(parsed_result, dict):
            is_single_sent = True
            parsed_result = [parsed_result]
        else:
            is_single_sent = False
        
        eventuality_rules = kw.get("eventuality_rules", None)
        if eventuality_rules is None:
            eventuality_rules = ALL_EVENTUALITY_RULES
        
        para_eventualities = [list() for _ in range(len(parsed_result))]
        for sent_parsed_result, sent_eventualities in zip(parsed_result, para_eventualities):
            # If it is a sentence that has clause word, just skip it
            # has_clause_word = False
            # for t in sent_parsed_result["tokens"]:
            #     if t in CLAUSE_WORDS:
            #         has_clause_word = True
            #         break
            # if has_clause_word:
            #     continue

            seed_rule_eventualities = dict()
            for rule_name in eventuality_rules:
                tmp_eventualities = self._extract_eventualities_from_dependencies_with_single_rule(
                    sent_parsed_result, eventuality_rules[rule_name], rule_name)
                seed_rule_eventualities[rule_name] = tmp_eventualities
            seed_rule_eventualities = self._filter_special_case(seed_rule_eventualities)
            for eventualities in seed_rule_eventualities.values():
                sent_eventualities.extend(eventualities)
        
        if in_order:
            if output_format == "json":
                para_eventualities = [[eventuality.encode(encoding=None) for Eventuality in sent_eventualities] \
                    for sent_eventualities in para_eventualities]
            if is_single_sent:
                return para_eventualities[0]
            else:
                return para_eventualities
        else:
            eid2eventuality = dict()
            for eventuality in chain.from_iterable(para_eventualities):
                eid = eventuality.eid
                if eid not in eid2eventuality:
                    eid2eventuality[eid] = copy(eventuality)
                else:
                    eid2eventuality[eid].update_frequency(eventuality)
            if output_format == "Eventuality":
                eventualities = sorted(eid2eventuality.values(), key=lambda e: e.eid)
            elif output_format == "json":
                eventualities = sorted([eventuality.encode(eventuality=None) for eventuality in eid2eventuality.values()], key=lambda e: e["eid"])
            return eventualities

    def _extract_eventualities_from_dependencies_with_single_rule(self, sent_parsed_result, eventuality_rule, rule_name):
        local_eventualities = list()
        verb_positions = [i for i, tag in enumerate(sent_parsed_result["pos_tags"])
                          if tag.startswith("VB")]
        for verb_position in verb_positions:
            tmp_e = self._extract_eventuality_with_fixed_target(
                sent_parsed_result, eventuality_rule, verb_position, rule_name)
            if tmp_e is not None:
                local_eventualities.append(tmp_e)
        return local_eventualities

    def _extract_eventuality_with_fixed_target(self, sent_parsed_result, eventuality_rule, verb_position, rule_name):
        selected_edges = list()
        selected_skeleton_edges = list()
        local_dict = {'V1': verb_position}
        for tmp_rule_r in eventuality_rule.positive_rules:
            foundmatch = False
            for dep_r in sent_parsed_result["dependencies"]:
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    selected_edges.append(dep_r)
                    selected_skeleton_edges.append(dep_r)
                    foundmatch = True
                    break
            if not foundmatch:
                # print('Miss one positive relation')
                return None

        for tmp_rule_r in eventuality_rule.possible_rules:
            for dep_r in sent_parsed_result["dependencies"]:
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    selected_edges.append(dep_r)

        for tmp_rule_r in eventuality_rule.negative_rules:
            for dep_r in sent_parsed_result["dependencies"]:
                if dep_r in selected_edges:
                    # print('This edge is selected by the positive example, so we will skip it')
                    continue
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    # print('found one negative relation')
                    return None

        if len(selected_edges) > 0:
            event = Eventuality(pattern=rule_name,
                                dependencies=selected_edges,
                                skeleton_dependencies=selected_skeleton_edges,
                                sent_parsed_results=sent_parsed_result)
            return event
        else:
            return None

    @staticmethod
    def _match_rule_r_and_dep_r(rule_r, dep_r, current_dict):
        tmp_dict = {key: val for key, val in current_dict.items()}
        if rule_r[1][0] == '-':
            tmp_relations = rule_r[1][1:].split('/')
            if rule_r[0] in current_dict and dep_r[0] == current_dict[rule_r[0]]:
                if dep_r[1] in tmp_relations:
                    return False, current_dict
                else:
                    # print(dep_r[1])
                    return True, tmp_dict
        if rule_r[1][0] == '+':
            tmp_relations = rule_r[1][1:].split('/')
            if rule_r[0] in current_dict and dep_r[0] == current_dict[rule_r[0]]:
                if dep_r[1] in tmp_relations:
                    tmp_dict[rule_r[2]] = dep_r[2]
                    return True, tmp_dict
                else:
                    # print(dep_r[1])
                    return False, current_dict
        if rule_r[1][0] == '^':
            tmp_dep_r = list()
            tmp_dep_r.append(dep_r[2])
            tmp_dep_r.append(dep_r[1])
            tmp_dep_r.append(dep_r[0])
            tmp_rule_r = list()
            tmp_rule_r.append(rule_r[2])
            tmp_rule_r.append(rule_r[1][1:])
            tmp_rule_r.append(rule_r[0])
            if tmp_rule_r[1] == tmp_dep_r[1]:
                if tmp_rule_r[0] in current_dict and tmp_dep_r[0] == current_dict[tmp_rule_r[0]]:
                    if tmp_rule_r[2] not in tmp_dict:
                        tmp_dict[tmp_rule_r[2]] = tmp_dep_r[2]
                        return True, tmp_dict
        else:
            tmp_dep_r = dep_r
            tmp_rule_r = rule_r
            if tmp_rule_r[1] == tmp_dep_r[1]:
                if tmp_rule_r[0] in current_dict and tmp_dep_r[0] == current_dict[tmp_rule_r[0]]:
                    if tmp_rule_r[2] not in tmp_dict:
                        tmp_dict[tmp_rule_r[2]] = tmp_dep_r[2]
                        return True, tmp_dict
        return False, current_dict

    @staticmethod
    def _filter_special_case(extracted_eventualities):
        extracted_eventualities['s-v-a'] = []
        extracted_eventualities['s-v-be-o'] = []

        if len(extracted_eventualities['s-v-v']) > 0:
            tmp_s_v_v = list()
            tmp_s_v_a = list()
            for e in extracted_eventualities['s-v-v']:
                for edge in e.dependencies:
                    if edge[1] == 'xcomp':
                        if 'VB' in edge[2][2]:
                            tmp_s_v_v.append(e)
                        if 'JJ' in edge[2][2]:
                            tmp_s_v_a.append(e)
                        break
            extracted_eventualities['s-v-v'] = tmp_s_v_v
            extracted_eventualities['s-v-a'] = tmp_s_v_a

        if len(extracted_eventualities['s-v-be-a']) > 0:
            tmp_s_v_be_a = list()
            tmp_s_v_be_o = list()
            for e in extracted_eventualities['s-v-be-a']:
                for edge in e.dependencies:
                    if edge[1] == 'xcomp':
                        if 'JJ' in edge[2][2]:
                            tmp_s_v_be_a.append(e)
                        if 'NN' in edge[2][2]:
                            tmp_s_v_be_o.append(e)
                        break
            extracted_eventualities['s-v-be-a'] = tmp_s_v_be_a
            extracted_eventualities['s-v-be-o'] = tmp_s_v_be_o

        if len(extracted_eventualities['s-v']) > 0:
            tmp_s_v = list()
            for e in extracted_eventualities['s-v']:
                for edge in e.dependencies:
                    if edge[1] == 'nsubj':
                        if edge[0][0] > edge[2][0] or edge[0][1] == 'be':
                            tmp_s_v.append(e)
            extracted_eventualities['s-v'] = tmp_s_v

        # Xin: has been replaced by removing "IN" when calling _construct
        # for relation in extracted_eventualities:
        #     new_eventualities = list()
        #     for tmp_e in extracted_eventualities[relation]:
        #         tmp_e._filter_dependency_by_word_list(CONNECTIVE_LIST, target="dependent")

        #         if len(tmp_e.dependencies) > 0:
        #             new_eventualities.append(tmp_e)
        #     extracted_eventualities[relation] = new_eventualities

        return extracted_eventualities

class DiscourseEventualityExtractor(BaseEventualityExtractor):
    def __init__(self, **kw):
        super().__init__(**kw)
        self.seed_rule_eventuality_extractor = SeedRuleEventualityExtractor(**kw)
        self.conn_extractor = ConnectiveExtractor(**kw)
        # self.argpos_classifier = ArgumentPositionClassifier(**kw)
        # self.ss_extractor = SSArgumentExtractor(**kw)
        # self.ps_extractor = PSArgumentExtractor(**kw)

    def extract_from_parsed_result(self, parsed_result, output_format="Eventuality", in_order=True, **kw):
        if output_format not in ["Eventuality", "json"]:
            raise NotImplementedError("Error: extract_from_parsed_result only supports Eventuality or json.")
        
        if not isinstance(parsed_result, (list, tuple, dict)):
            raise NotImplementedError
        if isinstance(parsed_result, dict):
            is_single_sent = True
            parsed_result = [parsed_result]
        else:
            is_single_sent = False

        syntax_tree_cache = kw.get("syntax_tree_cache", dict())
        
        para_eventualities = [list() for _ in range(len(parsed_result))]
        para_clauses = self._extract_clauses(parsed_result, syntax_tree_cache)
        for sent_parsed_result, sent_clauses, sent_eventualities in zip(parsed_result, para_clauses, para_eventualities):
            for clause in sent_clauses:
                idx_mapping = {j: i for i, j in enumerate(clause)}
                indices_set = set(clause)
                clause_parsed_result = {
                    "text": "",
                    "dependencies": [(idx_mapping[dep[0]], dep[1], idx_mapping[dep[2]]) for dep in sent_parsed_result["dependencies"] \
                        if dep[0] in indices_set and dep[2] in indices_set],
                    "tokens": [sent_parsed_result["tokens"][idx] for idx in clause],
                    "pos_tags": [sent_parsed_result["pos_tags"][idx] for idx in clause],
                    "lemmas": [sent_parsed_result["lemmas"][idx] for idx in clause]}
                eventualities = self.seed_rule_eventuality_extractor.extract_from_parsed_result(
                    clause_parsed_result, output_format="Eventuality", in_order=True)
                len_existed_eventualities = len(sent_eventualities)
                for e in eventualities:
                    for k, v in e.raw_sent_mapping.items():
                        e.raw_sent_mapping[k] = clause[v]
                    e.eid = Eventuality.generate_eid(e)
                    existed_eventuality = False
                    for e_idx in range(len_existed_eventualities):
                        if sent_eventualities[e_idx].eid == e.eid and \
                            sent_eventualities[e_idx].raw_sent_mapping == e.raw_sent_mapping:
                            existed_eventuality = True
                            break
                    if not existed_eventuality:
                        sent_eventualities.append(e)
        
        if in_order:
            if output_format == "json":
                para_eventualities = [[eventuality.encode(encoding=None) for Eventuality in sent_eventualities] \
                    for sent_eventualities in para_eventualities]
            if is_single_sent:
                return para_eventualities[0]
            else:
                return para_eventualities
        else:
            eid2eventuality = dict()
            for eventuality in chain.from_iterable(para_eventualities):
                eid = eventuality.eid
                if eid not in eid2eventuality:
                    eid2eventuality[eid] = copy(eventuality)
                else:
                    eid2eventuality[eid].update_frequency(eventuality)
            if output_format == "Eventuality":
                eventualities = sorted(eid2eventuality.values(), key=lambda e: e.eid)
            elif output_format == "json":
                eventualities = sorted([eventuality.encode(eventuality=None) for eventuality in eid2eventuality.values()], key=lambda e: e["eid"])
            return eventualities

    def _extract_clauses(self, parsed_result, syntax_tree_cache):
        para_arguments = [set() for _ in range(len(parsed_result))]
        connectives = self.conn_extractor.extract(parsed_result, syntax_tree_cache)
        para_connectives = [set() for _ in range(len(parsed_result))]
        for connective in connectives:
            sent_idx, indices = connective["sent_idx"], tuple(connective["indices"])
            para_connectives[sent_idx].add(indices)
        for sent_idx, sent_parsed_result in enumerate(parsed_result):
            sent_connectives = para_connectives[sent_idx]
            sent_arguments = para_arguments[sent_idx]

            if sent_idx in syntax_tree_cache:
                syntax_tree = syntax_tree_cache[sent_idx]
            else:
                syntax_tree = syntax_tree_cache[sent_idx] = SyntaxTree(sent_parsed_result["parse"])
            
            # the best but too slow
            for indices in powerset(sent_connectives):
                indices = set(chain.from_iterable(indices))
                sent_arguments.update(get_clauses(sent_parsed_result, syntax_tree, index_seps=indices))
            # sent_arguments.update(get_clauses(sent_parsed_result, syntax_tree, index_seps=set(chain.from_iterable(sent_connectives))))
        return para_arguments