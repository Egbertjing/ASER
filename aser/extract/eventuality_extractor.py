from aser.eventuality import Eventuality, EventualityList
from aser.extract.rule import ALL_EVENTUALITY_RULES, CONNECTIVE_LIST, CLAUSE_WORDS
from aser.extract.utils import get_corenlp_client, parse_sentense_with_stanford


class EventualityExtractor(object):
    def __init__(self, corenlp_path=None, corenlp_port=None):
        if corenlp_path and corenlp_port:
            self.corenlp_client, self.is_externel_corenlp = get_corenlp_client(
                corenlp_path=corenlp_path, port=corenlp_port)
        else:
            self.corenlp_client, self.is_externel_corenlp = None, False

    def close(self):
        if not self.is_externel_corenlp and self.corenlp_client:
            self.corenlp_client.stop()

    def __del__(self):
        self.close()

    def extract(self, text):
        """ This method would firstly split text into sentences and extract
            all eventualities for each sentence.

            :type text: str
            :param text: input text
            :return: a list of `EventualityList` object

            .. highlight:: python
            .. code-block:: python

                Input: 'The dog barks loudly. Because he is hungry.'

                Output:
                [
                EventualityList([
                    Eventuality({'dependencies': [((1, 'dog', 'NN'), 'det', (0, 'the', 'DT')),
                                                 ((2, 'bark', 'VBZ'), 'nsubj', (1, 'dog', 'NN')),
                                                 ((2, 'bark', 'VBZ'), 'advmod', (3, 'loudly', 'RB'))],
                                 'eid': 'c605e3d855d7d27cf25ed7e4d4e33962d9b10713',
                                 'pattern': 's-v',
                                 'pos_tags': ['DT', 'NN', 'VBZ', 'RB'],
                                 'skeleton_dependencies': [((2, 'bark', 'VBZ'), 'nsubj', (1, 'dog', 'NN'))],
                                 'skeleton_words': ['dog', 'bark'],
                                 'verbs': ['bark'],
                                 'words': ['the', 'dog', 'bark', 'loudly']})]),

                 EventualityList(
                    [Eventuality({'dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'he', 'PRP')),
                                                  ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBZ'))],
                                  'eid': 'dae842ef792d3ae786db5f71e36b50305dda14a4',
                                  'pattern': 's-be-a',
                                  'pos_tags': ['PRP', 'VBZ', 'JJ'],
                                  'skeleton_dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'he', 'PRP')),
                                                            ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBZ'))],
                                  'skeleton_words': ['he', 'be', 'hungry'],
                                  'verbs': ['be'],
                                  'words': ['he', 'be', 'hungry']})])
                ]
        """
        eventualities_list = []
        parsed_results = parse_sentense_with_stanford(text, self.corenlp_client)
        for parsed_result in parsed_results:
            eventualities = self.extract_from_parsed_result(parsed_result)
            eventualities_list.append(eventualities)
        return eventualities_list

    def extract_from_parsed_result(self, parsed_result):
        """ This method would extract eventualities from parsed_result of one sentence

        :type parsed_result: dict
        :param parsed_result: a dict generated by `aser.extract.utils.parse_sentense_with_stanford`
        :return: An `EventualityList` Object

        .. highlight:: python
        .. code-block:: python

            Input:
                {'dependencies': [(1, 'nsubj', 0),
                                  (1, 'nmod:to', 3),
                                  (1, 'advcl:because', 7),
                                  (1, 'punct', 8),
                                  (3, 'case', 2),
                                  (7, 'mark', 4),
                                  (7, 'nsubj', 5),
                                  (7, 'cop', 6)],
                 'lemma': ['I', 'go', 'to', 'lunch', 'because', 'I', 'be', 'hungry', '.'],
                 'pos_tags': ['PRP', 'VBP', 'TO', 'NN', 'IN', 'PRP', 'VBP', 'JJ', '.'],
                 'tokens': ['I', 'go', 'to', 'lunch', 'because', 'I', 'am', 'hungry', '.']}

            Output:
                EventualityList(
                    [Eventuality({'dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'I', 'PRP')),
                                      ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBP'))],
                                 'eid': 'eae8741fad51a57e78092017def1b5cb4f620d7e',
                                 'pattern': 's-be-a',
                                 'pos_tags': ['PRP', 'VBP', 'JJ'],
                                 'skeleton_dependencies': [((2, 'hungry', 'JJ'), 'nsubj', (0, 'I', 'PRP')),
                                                           ((2, 'hungry', 'JJ'), 'cop', (1, 'be', 'VBP'))],
                                 'skeleton_words': ['I', 'be', 'hungry'],
                                 'verbs': ['be'],
                                 'words': ['I', 'be', 'hungry']}),

                     Eventuality({'dependencies': [((1, 'go', 'VBP'), 'nsubj', (0, 'I', 'PRP')),
                                                  ((1, 'go', 'VBP'), 'nmod:to', (3, 'lunch', 'NN')),
                                                  ((3, 'lunch', 'NN'), 'case', (2, 'to', 'TO'))],
                                 'eid': '12b4aa577e56f2f5d96f4716bc97c633d6272ec4',
                                 'pattern': 's-v-X-o',
                                 'pos_tags': ['PRP', 'VBP', 'TO', 'NN'],
                                 'skeleton_dependencies': [((1, 'go', 'VBP'), 'nsubj', (0, 'I', 'PRP')),
                                                           ((1, 'go', 'VBP'), 'nmod:to', (3, 'lunch', 'NN')),
                                                           ((3, 'lunch', 'NN'), 'case', (2, 'to', 'TO'))],
                                 'skeleton_words': ['I', 'go', 'to', 'lunch'],
                                 'verbs': ['go'],
                                 'words': ['I', 'go', 'to', 'lunch']})]
                )
        """
        # If it is a sentence that has clause word, just skip it
        if set(parsed_result["tokens"]) & CLAUSE_WORDS:
            return EventualityList()
        all_eventualities = dict()
        for rule_name in ALL_EVENTUALITY_RULES:
            tmp_eventualities = self._extract_eventualities_from_dependencies_with_single_rule(
                parsed_result, ALL_EVENTUALITY_RULES[rule_name], rule_name)
            all_eventualities[rule_name] = tmp_eventualities
        all_eventualities = self._filter_special_case(all_eventualities)
        eventualities = EventualityList()
        for elist in all_eventualities.values():
            eventualities.extend(elist)
        return eventualities


    def _extract_eventualities_from_dependencies_with_single_rule(self, parsed_result, eventuality_rule,
                                                               rule_name):
        local_eventualities = EventualityList()
        verb_positions = [i for i, tag in enumerate(parsed_result["pos_tags"])
                          if tag.startswith("VB")]
        for verb_position in verb_positions:
            tmp_e = self._extract_eventuality_with_fixed_target(
                parsed_result, eventuality_rule, verb_position, rule_name)
            if tmp_e is not None:
                local_eventualities.append(tmp_e)
        return local_eventualities


    def _extract_eventuality_with_fixed_target(self, parsed_result, eventuality_rule, verb_position, rule_name):
        selected_edges = list()
        selected_skeleton_edges = list()
        local_dict = {'V1': verb_position}
        for tmp_rule_r in eventuality_rule.positive_rules:
            foundmatch = False
            for dep_r in parsed_result["dependencies"]:
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    selected_edges.append(dep_r)
                    selected_skeleton_edges.append(dep_r)
                    foundmatch = True
                    break
            if not foundmatch:
                # print('Miss one positive relation')
                return None

        for tmp_rule_r in eventuality_rule.possible_rules:
            for dep_r in parsed_result["dependencies"]:
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    selected_edges.append(dep_r)

        for tmp_rule_r in eventuality_rule.negative_rules:
            for dep_r in parsed_result["dependencies"]:
                if dep_r in selected_edges:
                    # print('This edge is selected by the positive example, so we will skip it')
                    continue
                decision, local_dict = self._match_rule_r_and_dep_r(tmp_rule_r, dep_r, local_dict)
                if decision:
                    # print('found one negative relation')
                    return None

        if len(selected_edges) > 0:
            event = Eventuality(pattern=rule_name,
                                dependencies=selected_edges,
                                skeleton_dependencies=selected_skeleton_edges,
                                sent_parsed_results=parsed_result)
            return event
        else:
            return None

    @staticmethod
    def _match_rule_r_and_dep_r(rule_r, dep_r, current_dict):
        tmp_dict = {key: val for key, val in current_dict.items()}
        if rule_r[1][0] == '-':
            tmp_relations = rule_r[1][1:].split('/')
            if rule_r[0] in current_dict and dep_r[0] == current_dict[rule_r[0]]:
                if dep_r[1] in tmp_relations:
                    return False, current_dict
                else:
                    # print(dep_r[1])
                    return True, tmp_dict
        if rule_r[1][0] == '+':
            tmp_relations = rule_r[1][1:].split('/')
            if rule_r[0] in current_dict and dep_r[0] == current_dict[rule_r[0]]:
                if dep_r[1] in tmp_relations:
                    tmp_dict[rule_r[2]] = dep_r[2]
                    return True, tmp_dict
                else:
                    # print(dep_r[1])
                    return False, current_dict
        if rule_r[1][0] == '^':
            tmp_dep_r = list()
            tmp_dep_r.append(dep_r[2])
            tmp_dep_r.append(dep_r[1])
            tmp_dep_r.append(dep_r[0])
            tmp_rule_r = list()
            tmp_rule_r.append(rule_r[2])
            tmp_rule_r.append(rule_r[1][1:])
            tmp_rule_r.append(rule_r[0])
            if tmp_rule_r[1] == tmp_dep_r[1]:
                if tmp_rule_r[0] in current_dict and tmp_dep_r[0] == current_dict[tmp_rule_r[0]]:
                    if tmp_rule_r[2] not in tmp_dict:
                        tmp_dict[tmp_rule_r[2]] = tmp_dep_r[2]
                        return True, tmp_dict
        else:
            tmp_dep_r = dep_r
            tmp_rule_r = rule_r
            if tmp_rule_r[1] == tmp_dep_r[1]:
                if tmp_rule_r[0] in current_dict and tmp_dep_r[0] == current_dict[tmp_rule_r[0]]:
                    if tmp_rule_r[2] not in tmp_dict:
                        tmp_dict[tmp_rule_r[2]] = tmp_dep_r[2]
                        return True, tmp_dict
        return False, current_dict

    @staticmethod
    def _filter_special_case(extracted_eventualities):
        extracted_eventualities['s-v-a'] = []
        extracted_eventualities['s-v-be-o'] = []
        if len(extracted_eventualities['s-v-v']) > 0:
            tmp_s_v_v = list()
            tmp_s_v_a = list()
            for e in extracted_eventualities['s-v-v']:
                for edge in e.dependencies:
                    if edge[1] == 'xcomp':
                        if 'VB' in edge[2][2]:
                            tmp_s_v_v.append(e)
                        if 'JJ' in edge[2][2]:
                            tmp_s_v_a.append(e)
                        break
            extracted_eventualities['s-v-v'] = tmp_s_v_v
            extracted_eventualities['s-v-a'] = tmp_s_v_a
        if len(extracted_eventualities['s-v-be-a']) > 0:
            tmp_s_v_be_a = list()
            tmp_s_v_be_o = list()
            for e in extracted_eventualities['s-v-be-a']:
                for edge in e.dependencies:
                    if edge[1] == 'xcomp':
                        if 'JJ' in edge[2][2]:
                            tmp_s_v_be_a.append(e)
                        if 'NN' in edge[2][2]:
                            tmp_s_v_be_o.append(e)
                        break
            extracted_eventualities['s-v-be-a'] = tmp_s_v_be_a
            extracted_eventualities['s-v-be-o'] = tmp_s_v_be_o
        if len(extracted_eventualities['s-v']) > 0:
            tmp_s_v = list()
            for e in extracted_eventualities['s-v']:
                for edge in e.dependencies:
                    if edge[1] == 'nsubj':
                        if edge[0][0] > edge[2][0] or edge[0][1] == 'be':
                            tmp_s_v.append(e)
            extracted_eventualities['s-v'] = tmp_s_v
        for relation in extracted_eventualities:
            new_eventualities = list()
            for tmp_e in extracted_eventualities[relation]:
                tmp_e._filter_dependency_by_word_list(CONNECTIVE_LIST, target="dependent")

                if len(tmp_e.dependencies) > 0:
                    new_eventualities.append(tmp_e)
            extracted_eventualities[relation] = new_eventualities

        return extracted_eventualities
